{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, os, string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, os, string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Tweet  label\n",
      "0                                                Tweet  label\n",
      "1       @handle Let's try and catch up live next week!   8746\n",
      "2    Going to watch Grey's on the big screen - Thur...   8746\n",
      "3    @handle My pleasure Patrick....hope you are well!   8746\n",
      "4    @handle Hi there! Been traveling a lot and lot...   8746\n",
      "5    RT @handle Looking to Drink Clean & Go Green? ...   8746\n",
      "6    RT @handle: Ft. Hood officials confirm the 2 o...   8746\n",
      "7    RT @handle: Mickey Mouse is Getting a Make Ove...   8746\n",
      "8             @handle How did u get the invite Justin?   8746\n",
      "9    @handle I think I am still a good friend of he...   8746\n",
      "10   @handle I remember! I am fine - how are u? Wha...   8746\n",
      "11      @handle That's great - good for the coach!!!!!   8746\n",
      "12   @handle I don't want to picture u sitting on i...   8746\n",
      "13   @handle D- Thanks for the RTs....are you going...   8746\n",
      "14            @handle Grrr....you must be going crazy!   8746\n",
      "15   @handle Hi there - just catching up from my tr...   8746\n",
      "16   RT @handle: If you're looking for some great l...   8746\n",
      "17   RT @handle: Retailers who arent engaging custo...   8746\n",
      "18   Still in car....want to jump out....45 minutes...   8746\n",
      "19   RT @handle: \"Only surround yourself with peopl...   8746\n",
      "20   @handle wish I could but 24/7 w stu's family t...   8746\n",
      "21   RT @handle: Help us help MusiCares! Vote for C...   8746\n",
      "22                      @handle yum!!!! Save me some!!   8746\n",
      "23   RT @handle: Gratitude is the sign of noble sou...   8746\n",
      "24            @handle I don't think I know what it is!   8746\n",
      "25   RT @handle: @handle Just found you via @handle...   8746\n",
      "26   RT @handle: RT @handle: Travelling for the Hol...   8746\n",
      "27        Just entering ohio - special hi to @handle!!   8746\n",
      "28   @handle well we agree on one food thing friend...   8746\n",
      "29                                 @handle only 1!!!!!   8746\n",
      "..                                                 ...    ...\n",
      "194  Consumer Reports Gives Nod to Ford Reliability...   9661\n",
      "195  Connect with us and be part of the Ford Story ...   9661\n",
      "196  Ford to unveil custom Mustangs at SEMA http://...   9661\n",
      "197  Drive One Shifts Gears to Add Customer Reactio...   9661\n",
      "198  What's the Diff? Ford F-150 SVT Raptor vs. Dod...   9661\n",
      "199      A V6 Engine Like no Other http://bit.ly/QJWLY   9661\n",
      "200  RT @handle We are working 4 Ford dealers on th...   9661\n",
      "201  RT @handle Did u know:texting on a cell phone ...   9661\n",
      "202  Due to increasing job duties, I can't tweet 6-...   9661\n",
      "203  Ford Hauls Home a Host of Awards From Texas ht...   9661\n",
      "204  Sorry for the absence; I had to move my office...   9661\n",
      "205                                  @handle Awesome!!   9661\n",
      "206                                 @handle Too funny.   9661\n",
      "207                         @handle Watching what? LOL   9661\n",
      "208                           @handle You are welcome!   9661\n",
      "209  Frozen, baked, rattled, stoned  Ford Taurus en...   9661\n",
      "210  Alan Mulally to tout SYNC, new surprises in ke...   9661\n",
      "211  The curious case of the quietest debuting dies...   9661\n",
      "212  New Taurus balances the 'bling' to project jus...   9661\n",
      "213  Survey says: Texting while driving 'very dange...   9661\n",
      "214  2011 Super Duty: significantly improved torque...   9661\n",
      "215  We have some great \"Save The Ta-Tas\" merchandi...   9661\n",
      "216  We are still raising breast cancer awareness a...   9661\n",
      "217                         @handle thanks for the RT!   9661\n",
      "218                                        @handle LOL   9661\n",
      "219  RT @handle~ @handle You'll love this - Deal or...   9661\n",
      "220  I haven't had time to twitter since I've been ...   9661\n",
      "221  Come see us at the Rockdale County Fair! We ar...   9661\n",
      "222            @handle @handle Thanks for the mention!   9661\n",
      "223  GET COUPONS FOR SERVICE! http://bit.ly/oMvn/Pa...   9661\n",
      "\n",
      "[224 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = None\n",
    "test_data = None\n",
    "\n",
    "def load_data():\n",
    "    global train_data, test_data\n",
    "    train_data = pd.read_csv('split_data_original.txt', delimiter=\"\\t\", header=None)\n",
    "    test_data = pd.read_csv('test_tweets_unlabeled.txt', delimiter=\"\\t\", header = None)\n",
    "    \n",
    "load_data()\n",
    "train_data.columns = ['Tweet', 'label']\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    documents_f = open(filepath, 'rb')\n",
    "    file = pickle.load(documents_f)\n",
    "    documents_f.close()\n",
    "    \n",
    "    return file\n",
    "\n",
    "def save_pickle(data, filepath):\n",
    "    save_documents = open(filepath, 'wb')\n",
    "    pickle.dump(data, save_documents)\n",
    "    save_documents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp = [word for word in list(stopwords.words('english') + [ \"'s\", \"'m\", \"ca\"])\n",
    "        if word not in [\"no\", \"not\"] and word.rfind(\"n't\") == -1]\n",
    "\n",
    "class PreProcessor(object):\n",
    "    '''Pre-processor which cleans text, lemmatises, removes stop words and punctuation, \n",
    "    returns df of processed text.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self._stopWordList = stp\n",
    "        self._punct_removal = list(string.punctuation)\n",
    "        self.sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    def remove_url(self, text):\n",
    "        result = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return result\n",
    "    \n",
    "    def check_url(self, text):\n",
    "        url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text) \n",
    "        if len(url) != 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def unique_words(self, words):\n",
    "        word_count = len(words)\n",
    "        unique_count = len(set(words))\n",
    "        if word_count!=0:\n",
    "            return unique_count / word_count\n",
    "        return 0\n",
    "    \n",
    "    def find_tags(self, text):\n",
    "        from nltk.tag import StanfordNERTagger\n",
    "        from nltk import word_tokenize\n",
    "        \n",
    "        jar = '/Users/belle/Downloads/stanford-ner/stanford-ner.jar'\n",
    "        model = '/Users/belle/Downloads/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "        \n",
    "        st = StanfordNERTagger(model, jar, encoding='utf8')\n",
    "\n",
    "        for i in range(1):\n",
    "            classified_text = st.tag(word_tokenize(text))\n",
    "            print(classified_text)\n",
    "        return classified_text\n",
    "    \n",
    "    \n",
    "   \n",
    "    def transform_text1(self, data):\n",
    "        no_punct_translator=str.maketrans('','',string.punctuation)\n",
    "        data['words'] = data['Tweet'].apply(lambda row: self.remove_url(str(row))).apply(lambda t: nltk.word_tokenize(t.translate(no_punct_translator).lower()))\n",
    "        #print(\"data['words']=\" + str(data['words']))\n",
    "        #aa = [data['words']]\n",
    "        #print(\"aa=\" + str(aa))\n",
    "        \n",
    "        #print(\"data=\" + str(data))\n",
    "        \n",
    "        \n",
    "        ### tokenized\n",
    "        tokenizedWord = sum(data['words'],[])\n",
    "        #print(\"tokenizedWord=\" + str(tokenizedWord))\n",
    "        \n",
    "        ### remove stop word\n",
    "        rmStopword = [w for w in tokenizedWord if not w in stp]\n",
    "        \n",
    "        ### connecting all filtered words\n",
    "        all_words = ' '.join([text for text in rmStopword])\n",
    "        all_list= [' '.join([text for text in rmStopword])]\n",
    "        #print(\"all_words=\"+all_words)\n",
    "        #print(\"all_list=\"+str(all_list))\n",
    "      \n",
    "        ### create the word cloud\n",
    "        from wordcloud import WordCloud\n",
    "        wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()    \n",
    "        \n",
    "        ### count the word frequency\n",
    "        a = nltk.FreqDist(rmStopword)\n",
    "        d = pd.DataFrame({\"Word\": list(a.keys()),\n",
    "                          \"Count\": list(a.values())})\n",
    "        # selecting top 10 most frequency      \n",
    "        d = d.nlargest(columns=\"Count\", n = 20) \n",
    "        plt.figure(figsize=(16,5))\n",
    "        ax = sns.barplot(data=d, x= \"Word\", y = \"Count\")\n",
    "        ax.set(ylabel = \"Count\")\n",
    "        plt.show()\n",
    "             \n",
    "        ### bag-of-words feature matrix\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        \n",
    "        #aa = [\" \".join(context) for context in data['words'].values]\n",
    "        aa = [\" \".join(context) for context in data['words'].values]\n",
    "        #print(\"aa=\" + str(aa))\n",
    "       \n",
    "        bow_vectorizer = CountVectorizer(max_features=10, stop_words='english')\n",
    "        bow = bow_vectorizer.fit_transform(aa) \n",
    "        bowArray = bow.toarray()\n",
    "        #get_feature_names()可获取词袋中所有文本的关键字\n",
    "        #toarray()可看到词频矩阵的结果\n",
    "        print(bowArray)\n",
    "        print (bow_vectorizer.get_feature_names())\n",
    "    \n",
    " \n",
    "        ### TF-IDF\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=10, stop_words='english')\n",
    "        # TF-IDF feature matrix\n",
    "        tfidf = tfidf_vectorizer.fit_transform(aa)\n",
    "        \n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3c94abd79466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_text1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclean_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a0116b0b8002>\u001b[0m in \u001b[0;36mtransform_text1\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m### create the word cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "processor = PreProcessor()\n",
    "clean_train = processor.transform_text1(train_data)\n",
    "clean_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/yunqiangpu/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yunqiangpu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
